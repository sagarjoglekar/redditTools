{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:25.589108Z",
     "start_time": "2020-09-18T13:52:25.578576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "from glob import glob\n",
    "import pandas as pd \n",
    "import pickle as pkl\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import json\n",
    "import requests as rq\n",
    "import time\n",
    "import lzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:25.661827Z",
     "start_time": "2020-09-18T13:52:25.654795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='Parsing.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:26.956040Z",
     "start_time": "2020-09-18T13:52:26.952806Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump = \"/10TBdrive/datashare/Reddit/submissions/RS_2017-10.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:27.366041Z",
     "start_time": "2020-09-18T13:52:27.361654Z"
    }
   },
   "outputs": [],
   "source": [
    "openedDump = bz2.BZ2File(dump, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:27.743031Z",
     "start_time": "2020-09-18T13:52:27.736562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions_bz = glob(\"/10TBdrive/datashare/Reddit/submissions/*.bz2\")\n",
    "submissions_xz = glob(\"/10TBdrive/datashare/Reddit/submissions/*.xz\")\n",
    "\n",
    "submissions = submissions_bz + submissions_xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:28.231449Z",
     "start_time": "2020-09-18T13:52:28.228509Z"
    }
   },
   "outputs": [],
   "source": [
    "# submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:28.908628Z",
     "start_time": "2020-09-18T13:52:28.866084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'archived': False, 'author': 'SealTeamSith', 'author_flair_css_class': 'red', 'author_flair_text': 'Transgender', 'brand_safe': False, 'contest_mode': False, 'created_utc': 1506816000, 'distinguished': None, 'domain': 'self.RightwingLGBT', 'edited': False, 'gilded': 0, 'hidden': False, 'hide_score': False, 'id': '73ig7j', 'is_crosspostable': False, 'is_reddit_media_domain': False, 'is_self': True, 'is_video': False, 'link_flair_css_class': 'rainbow', 'link_flair_text': 'GO VOTE!', 'locked': False, 'media': None, 'media_embed': {}, 'num_comments': 20, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': None, 'permalink': '/r/RightwingLGBT/comments/73ig7j/im_going_to_be_applying_some_css_changes_this/', 'pinned': False, 'retrieved_on': 1510382444, 'score': 7, 'secure_media': None, 'secure_media_embed': {}, 'selftext': '', 'spoiler': False, 'stickied': False, 'subreddit': 'RightwingLGBT', 'subreddit_id': 't5_3dc8i', 'suggested_sort': None, 'thumbnail': 'self', 'thumbnail_height': None, 'thumbnail_width': None, 'title': \"I'm going to be applying some CSS changes this evening. If something isn't rendering correctly then please post here.\", 'url': 'https://www.reddit.com/r/RightwingLGBT/comments/73ig7j/im_going_to_be_applying_some_css_changes_this/', 'whitelist_status': None}\n",
      "{'archived': False, 'author': '[deleted]', 'author_flair_css_class': None, 'author_flair_text': None, 'brand_safe': True, 'contest_mode': False, 'created_utc': 1506816000, 'distinguished': None, 'domain': 'self.edc_raffle', 'edited': 1506818201.0, 'gilded': 0, 'hidden': False, 'hide_score': False, 'id': '73ig7k', 'is_crosspostable': False, 'is_reddit_media_domain': False, 'is_self': True, 'is_video': False, 'link_flair_css_class': 'complete', 'link_flair_text': 'Complete', 'locked': False, 'media': None, 'media_embed': {}, 'num_comments': 63, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/edc_raffle/comments/73ig7k/omega_seamaster_aqua_terra_quartz_65_spots_at/', 'pinned': False, 'retrieved_on': 1510382444, 'score': 9, 'secure_media': None, 'secure_media_embed': {}, 'selftext': '[deleted]', 'spoiler': False, 'stickied': False, 'subreddit': 'edc_raffle', 'subreddit_id': 't5_3jcb6', 'suggested_sort': 'new', 'thumbnail': 'default', 'thumbnail_height': None, 'thumbnail_width': None, 'title': 'Omega Seamaster Aqua Terra Quartz - 65 spots at $20/ea with NO spot limit.', 'url': 'https://www.reddit.com/r/edc_raffle/comments/73ig7k/omega_seamaster_aqua_terra_quartz_65_spots_at/', 'whitelist_status': 'all_ads'}\n",
      "{'archived': False, 'author': '[deleted]', 'author_flair_css_class': None, 'author_flair_text': None, 'brand_safe': True, 'contest_mode': False, 'created_utc': 1506816000, 'distinguished': None, 'domain': 'self.RocketLeague', 'edited': False, 'gilded': 0, 'hidden': False, 'hide_score': False, 'id': '73ig7l', 'is_crosspostable': False, 'is_reddit_media_domain': False, 'is_self': True, 'is_video': False, 'link_flair_css_class': None, 'link_flair_text': None, 'locked': False, 'media': None, 'media_embed': {}, 'num_comments': 0, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/RocketLeague/comments/73ig7l/matchmaking_doesnt_find_me_teammates_for_whole/', 'pinned': False, 'retrieved_on': 1510382444, 'score': 1, 'secure_media': None, 'secure_media_embed': {}, 'selftext': '[deleted]', 'spoiler': False, 'stickied': False, 'subreddit': 'RocketLeague', 'subreddit_id': 't5_30cz1', 'suggested_sort': None, 'thumbnail': 'default', 'thumbnail_height': None, 'thumbnail_width': None, 'title': \"Matchmaking doesn't find me teammates for whole games even when there's 122k players online with 7.4k in the playlist.\", 'url': 'https://www.reddit.com/r/RocketLeague/comments/73ig7l/matchmaking_doesnt_find_me_teammates_for_whole/', 'whitelist_status': 'all_ads'}\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "for line in openedDump:\n",
    "    if lines > 2:\n",
    "        break\n",
    "    lineData = json.loads(line.strip())\n",
    "    print(lineData)\n",
    "    lines+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:52:29.742631Z",
     "start_time": "2020-09-18T13:52:29.646772Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSubRedditMapping(filePaths , limit=None , comments_thresh=10):\n",
    "    import json\n",
    "    import bz2\n",
    "    import lzma\n",
    "    returnData = dict()\n",
    "    file_count = 0\n",
    "    total = len(filePaths)\n",
    "    for path in tqdm(filePaths):\n",
    "        extension = path.split('.')[-1]\n",
    "        if extension == 'bz2':\n",
    "            openedDump = bz2.BZ2File(path, \"r\")\n",
    "        else:\n",
    "            openedDump = lzma.open(path)\n",
    "        count = 0\n",
    "        print(\"=============Parsing============\")\n",
    "        logging.debug(\"=============Parsing============\")\n",
    "        st = time.time()\n",
    "        for line in openedDump:\n",
    "            try:\n",
    "                lineData = json.loads(line.strip())\n",
    "                if lineData['num_comments'] >= comments_thresh:\n",
    "                    sub_id = lineData['id']\n",
    "                    subreddit = lineData['subreddit']\n",
    "                    subreddit_id = lineData['subreddit_id']\n",
    "                    returnData[sub_id] = {}\n",
    "                    returnData[sub_id]['subName'] = subreddit\n",
    "                    returnData[sub_id]['subId'] = subreddit_id\n",
    "            except:\n",
    "                logging.warn(f\"Incomplete record found\")\n",
    "            if count%1000000 == 0:\n",
    "                end = time.time()\n",
    "                delta = end-st\n",
    "                print(\"Done parsing %d posts in %d seconds\",(count,delta))\n",
    "                \n",
    "                logging.debug(f\"Done parsing {count} posts in {delta} seconds\")\n",
    "                st = time.time()\n",
    "            count+=1\n",
    "        openedDump.close()\n",
    "        \n",
    "        logging.debug(f\"Done {file_count} out of  {total} Files \")\n",
    "        file_count+=1\n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-18T13:52:30.527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/109 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.035041093826293945)\n",
      "Done parsing %d posts in %d seconds (1000000, 63.67950677871704)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/109 [01:08<2:04:09, 68.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.014872550964355469)\n",
      "Done parsing %d posts in %d seconds (1000000, 65.85064244270325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/109 [03:18<2:35:30, 87.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.016068458557128906)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing %d posts in %d seconds (1000000, 99.63058614730835)\n",
      "Done parsing %d posts in %d seconds (2000000, 99.04214477539062)\n",
      "Done parsing %d posts in %d seconds (3000000, 107.37112092971802)\n",
      "Done parsing %d posts in %d seconds (4000000, 99.87625861167908)\n",
      "Done parsing %d posts in %d seconds (5000000, 99.91255164146423)\n",
      "Done parsing %d posts in %d seconds (6000000, 100.01333355903625)\n",
      "Done parsing %d posts in %d seconds (7000000, 100.28596425056458)\n",
      "Done parsing %d posts in %d seconds (8000000, 100.65285563468933)\n",
      "Done parsing %d posts in %d seconds (9000000, 99.89277601242065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/109 [19:25<10:20:15, 351.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.03770256042480469)\n",
      "Done parsing %d posts in %d seconds (1000000, 71.39521169662476)\n",
      "Done parsing %d posts in %d seconds (2000000, 72.4762282371521)\n",
      "Done parsing %d posts in %d seconds (3000000, 73.40321159362793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 4/109 [23:12<9:09:16, 313.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.016691923141479492)\n",
      "Done parsing %d posts in %d seconds (1000000, 72.90861535072327)\n",
      "Done parsing %d posts in %d seconds (2000000, 76.5642762184143)\n",
      "Done parsing %d posts in %d seconds (3000000, 80.54893350601196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 5/109 [27:22<8:31:00, 294.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.03621268272399902)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/109 [28:27<6:27:19, 225.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.026987075805664062)\n",
      "Done parsing %d posts in %d seconds (1000000, 96.15116810798645)\n",
      "Done parsing %d posts in %d seconds (2000000, 99.74843788146973)\n",
      "Done parsing %d posts in %d seconds (3000000, 108.80387353897095)\n",
      "Done parsing %d posts in %d seconds (4000000, 101.96373987197876)\n",
      "Done parsing %d posts in %d seconds (5000000, 96.19396734237671)\n",
      "Done parsing %d posts in %d seconds (6000000, 107.94531559944153)\n",
      "Done parsing %d posts in %d seconds (7000000, 96.45985317230225)\n",
      "Done parsing %d posts in %d seconds (8000000, 104.73147630691528)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 7/109 [43:00<11:53:44, 419.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.052877187728881836)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 8/109 [43:58<8:44:08, 311.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.015622854232788086)\n",
      "Done parsing %d posts in %d seconds (1000000, 106.4487247467041)\n",
      "Done parsing %d posts in %d seconds (2000000, 115.0202808380127)\n",
      "Done parsing %d posts in %d seconds (3000000, 112.52869153022766)\n",
      "Done parsing %d posts in %d seconds (4000000, 110.80644369125366)\n",
      "Done parsing %d posts in %d seconds (5000000, 118.31415438652039)\n",
      "Done parsing %d posts in %d seconds (6000000, 119.8276674747467)\n",
      "Done parsing %d posts in %d seconds (7000000, 111.9313793182373)\n",
      "Done parsing %d posts in %d seconds (8000000, 103.45414733886719)\n",
      "Done parsing %d posts in %d seconds (9000000, 117.87832427024841)\n",
      "Done parsing %d posts in %d seconds (10000000, 116.69969964027405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 9/109 [1:03:25<15:46:37, 567.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.017242431640625)\n",
      "Done parsing %d posts in %d seconds (1000000, 74.19068479537964)\n",
      "Done parsing %d posts in %d seconds (2000000, 73.0602433681488)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 10/109 [1:06:49<12:37:03, 458.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.016691207885742188)\n",
      "Done parsing %d posts in %d seconds (1000000, 77.66080641746521)\n",
      "Done parsing %d posts in %d seconds (2000000, 78.53942060470581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 11/109 [1:10:09<10:22:49, 381.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.015614748001098633)\n",
      "Done parsing %d posts in %d seconds (1000000, 70.96942186355591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 12/109 [1:11:55<8:02:50, 298.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.016551971435546875)\n",
      "Done parsing %d posts in %d seconds (1000000, 74.20077443122864)\n",
      "Done parsing %d posts in %d seconds (2000000, 70.68603205680847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 13/109 [1:14:47<6:56:59, 260.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Parsing============\n",
      "Done parsing %d posts in %d seconds (0, 0.03762054443359375)\n",
      "Done parsing %d posts in %d seconds (1000000, 72.8566620349884)\n"
     ]
    }
   ],
   "source": [
    "graph_to_sub_map = getSubRedditMapping(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-18T13:54:43.961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store graph_to_sub_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:06:09.178933Z",
     "start_time": "2020-09-17T16:05:38.548547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph_to_sub_map = {}\n",
    "# for s in submissions:\n",
    "#     sub = pkl.load(open(s,'rb'))\n",
    "#     for k in sub:\n",
    "#         subreddit = sub[k].split('/')[2] j\n",
    "#         graph_to_sub_map[k] = subreddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:06:15.648123Z",
     "start_time": "2020-09-17T16:06:14.908724Z"
    }
   },
   "outputs": [],
   "source": [
    "# {k: graph_to_sub_map[k] for k in list(graph_to_sub_map)[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T09:59:07.124788Z",
     "start_time": "2020-09-18T09:59:00.427Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphs = glob('/10TBdrive/datashare/Reddit/graphs/*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:06:20.904249Z",
     "start_time": "2020-09-17T16:06:20.899632Z"
    }
   },
   "outputs": [],
   "source": [
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:20:35.779681Z",
     "start_time": "2020-09-17T16:20:35.765976Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = pkl.load(open(graphs[10],'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:20:42.587089Z",
     "start_time": "2020-09-17T16:20:42.582127Z"
    }
   },
   "outputs": [],
   "source": [
    "graphs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:20:48.730623Z",
     "start_time": "2020-09-17T16:20:48.723931Z"
    }
   },
   "outputs": [],
   "source": [
    "g.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T16:06:43.145903Z",
     "start_time": "2020-09-17T16:06:43.143104Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-03T14:02:06.163Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/10TBdrive/datashare/Reddit/AllParsedData_2005_to_2012_and_2017.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "    w = csv.DictWriter(f, ['author','downs','ups','text','time','src_id','dest_id','dest_author','depth','subreddit'],delimiter='|')\n",
    "    w.writeheader()\n",
    "    for gra in tqdm(graphs):\n",
    "        g = pkl.load(open(gra,'rb'))\n",
    "        gid = gra.split('/')[-1].split('.')[0]\n",
    "        try:\n",
    "            subreddit = graph_to_sub_map[gid]\n",
    "            nodes = g.nodes(data=True)\n",
    "            node_dict = {n[0] : n[1]  for n in nodes}\n",
    "            for u , v in g.edges():\n",
    "                if node_dict[u]['author']!='[deleted]' and node_dict[v]['author']!='[deleted]' :\n",
    "                    row= {'author': node_dict[u]['author'] , 'downs' : node_dict[u]['downs'] , 'ups': node_dict[u]['ups'] , 'text':node_dict[u]['text'] , 'time':node_dict[u]['time'] , 'src_id' : u , 'dest_id' : v , 'dest_author' : node_dict[v]['author'] ,'depth':node_dict[u]['depth'],'subreddit':subreddit }\n",
    "                    w.writerow(row)\n",
    "                else:\n",
    "                    continue\n",
    "        except:\n",
    "            print(\"Some graph is malformed!! \")\n",
    "            logging.debug(\"Some graph is malformed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reddit_data = pd.read_csv('/10TBdrive/datashare/Reddit/AllParsedData_2017_V3.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T09:59:22.037815Z",
     "start_time": "2020-04-06T09:59:22.035007Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reddit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:00:11.990165Z",
     "start_time": "2020-04-06T10:00:11.982483Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(reddit_data['author'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
